# üõ†Ô∏è Strategy 1: AI-Augmented Projects as a Distinct Category

**The Problem**: Current policy forces binary choices‚Äîeither AI is "allowed" or it isn't. This doesn't reflect how AI will be used professionally.

**The Solution**: Create a new assessment category where AI use is expected and students are graded on their orchestration skill.

**AI-Augmented Project Rubric**:

| Criterion | Developing (1) | Proficient (2) | Exemplary (3) |
|-----------|----------------|----------------|---------------|
| **Prompt Quality** | Basic prompts, minimal iteration | Clear prompts with evidence of refinement | Sophisticated prompt architecture with systematic iteration |
| **Verification** | Limited fact-checking | Systematic verification of key claims | Comprehensive verification with methodology documentation |
| **Human Contribution** | Unclear what student added | Clear evidence of human judgment and refinement | Significant human insight, creativity, or analysis beyond AI capability |
| **Process Documentation** | Minimal documentation | Clear record of AI interactions and decisions | Detailed portfolio showing evolution of thinking and approach |
| **Ethical Consideration** | Limited awareness | Identifies key ethical considerations | Thoughtful analysis with stakeholder awareness |

**Implementation Note**: Not all projects should be AI-augmented. The skill is knowing when AI helps and when it doesn't. A mix of AI-augmented, AI-optional, and AI-free assignments develops this judgment.

### Maximizing Agency: Student-Led Tech Stacks
To maximize agency, allow students to **"Pitch Their Tech Stack."** Instead of prescribing specific tools, encourage students to propose their own toolchain (e.g., "I used Perplexity for research and Claude for outlining"). Assessment should weigh the **Process Artifact** (the orchestration log) equally with the final output, as the dialogue with the AI is where the cognitive work is visible.
